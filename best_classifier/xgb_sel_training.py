# -*- coding: utf-8 -*-
"""XGB_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iqXW2NGTCXy53_Ct8lS8FtZyVPdUSPxb
"""

from sklearn.model_selection import train_test_split
import xgboost as xgb
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score, recall_score, f1_score
import pandas as pd
import numpy as np
import nltk
from xgboost import XGBClassifier
#from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import pandas as pd
import xgboost as xgb
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import MultinomialNB
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.svm import SVC
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.model_selection import train_test_split
import xgboost as xgb

from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.datasets import fetch_20newsgroups

labeled_data = pd.read_excel('/content/sample_data/labeled_data.xlsx')
unlabeled_data = pd.read_excel('/content/sample_data/unlabeled_data.xlsx')
#add=pd.read_excel('/content/sample_data/unlabaled_data_for_add.xlsx')

labeled_data = labeled_data.dropna(subset=["sentiment"])
labeled_data = labeled_data[np.isfinite(labeled_data["sentiment"])]
# Use astype() function to cast the column to integer type
labeled_data["sentiment"] = labeled_data["sentiment"].astype(int)

X_labeled_train, X_labeled_test, y_labeled_train, y_labeled_test = train_test_split(labeled_data['rev_clean'], labeled_data['sentiment'], test_size=0.2, random_state=42)

labeled_reviews = X_labeled_train  # List of labeled car reviews
labeled_labels = y_labeled_train  # List of corresponding labels (0 or 1)
import nltk
nltk.download('stopwords')
stop_wordss = set(stopwords.words('french'))
# Load the unlabeled dataset
unlabeled_reviews = unlabeled_data['rev_clean']

# Vectorize the text data
vectorizer = TfidfVectorizer(stop_words=list(stop_wordss))
labeled_reviews_vectorized = vectorizer.fit_transform(labeled_reviews)

from sklearn.model_selection import GridSearchCV
from xgboost import XGBClassifier

"""# Define the parameter grid for tuning
param_grid = {
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 4, 5, 6],
    'n_estimators': [500, 800, 1000, 1200]
}"""

# Create an XGBoost classifier
#xgb = XGBClassifier()

# Perform grid search cross-validation
#grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=5, scoring='accuracy')

"""grid_search.fit(labeled_reviews_vectorized, labeled_labels) # Replace X and y with your data
print("Best Parameters: ", grid_search.best_params_)
print("Best Score: ", grid_search.best_score_)"""

xgb = XGBClassifier(learning_rate = 0.1, max_depth = 4, n_estimators = 600)
#xgb = XGBClassifier(learning_rate = 0.1, max_depth = 5, n_estimators = 800)

#xgb = XGBClassifier(learning_rate = 0.1, max_depth = 3, n_estimators = 100,objective='binary:logistic')

xgb.fit(labeled_reviews_vectorized, labeled_labels)

num_iterations = 10
threshold_confidence = 0.9

unlabeled_reviews_vectorized = vectorizer.transform(unlabeled_reviews)
# Make predictions on unlabeled data
pseudo_labels = xgb.predict(unlabeled_reviews_vectorized)
pseudo_probabilities = xgb.predict_proba(unlabeled_reviews_vectorized)

# Filter pseudo-labels with high confidence
high_confidence_indices = np.where(np.max(pseudo_probabilities, axis=1) >= threshold_confidence)[0]

# Add high confidence samples to labeled data and remove from unlabeled data
labeled_reviews = pd.concat([labeled_reviews, pd.Series([unlabeled_reviews[i] for i in high_confidence_indices])], ignore_index=True)
labeled_labels = pd.concat([labeled_labels, pd.Series(pseudo_labels[high_confidence_indices])], ignore_index=True)
unlabeled_reviews_up = [review for i, review in enumerate(unlabeled_reviews) if i not in high_confidence_indices]

# Retrain the model with the combined labeled dataset
labeled_reviews_vectorized = vectorizer.transform(labeled_reviews)
xgb.fit(labeled_reviews_vectorized, labeled_labels)

#####prediction
unlabeled_reviews_vec = vectorizer.transform(unlabeled_reviews_up)
y_unlabeled_pred = xgb.predict(unlabeled_reviews_vec)

x=vectorizer.transform(X_labeled_test)
y_labeled_pred = xgb.predict(x)

# Evaluating the accuracy of the model trained with labeled data
print("Accuracy de donn√©es labeled:", accuracy_score(y_labeled_test, y_labeled_pred))
precision_labeled = precision_score(y_labeled_test, y_labeled_pred, average='weighted')
recall_labeled = recall_score(y_labeled_test, y_labeled_pred, average='weighted')
f1_labeled = f1_score(y_labeled_test, y_labeled_pred, average='weighted')
print("Precision Labeled Data :", precision_labeled)
print("Recall Labeled Data :", recall_labeled)
print("F1-Score Labeled Data :", f1_labeled)

import pickle
# Save the object using protocol version 4
with open('model_best.pkl', 'wb') as file:
    pickle.dump(xgb, file, protocol=4)

import pickle
# Save the object using protocol version 4
with open('vectorizer_best.pkl', 'wb') as file:
    pickle.dump(vectorizer, file, protocol=4)