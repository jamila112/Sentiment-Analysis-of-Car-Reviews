# -*- coding: utf-8 -*-
"""XGB_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AwPzIjNCuGWtkMyXrzfln8GcrCZtYszi
"""

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
import xgboost as xgb
from sklearn.metrics import accuracy_score
import numpy as np
import pandas as pd

# Import necessary libraries
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import MultinomialNB
import re

labeled_data = pd.read_excel('/content/sample_data/labeled_data.xlsx')
unlabeled_data = pd.read_excel('/content/sample_data/unlabeled_data.xlsx')

labeled_data = labeled_data.dropna(subset=["sentiment"])
labeled_data = labeled_data[np.isfinite(labeled_data["sentiment"])]
# Use astype() function to cast the column to integer type
labeled_data["sentiment"] = labeled_data["sentiment"].astype(int)

files=unlabeled_data[:1000]



files.to_excel("data_api.xlsx")

# Split labeled data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(labeled_data['rev_clean'], labeled_data['sentiment'], test_size=0.2, random_state=42)

X1_labeledd = X_train[:1202]  # Labeled data for view 1 (text reviews)
X2_labeledd = X_train[1202:]   # Labeled data for view 2 (additional information about the reviews)

y_labeled1 = y_train[:1202]  # Labeled data for view 1 (text reviews)
y_labeled2 = y_train[1202:]

data_unlabeled1 = unlabeled_data[:33013]  # Labeled data for view 1 (text reviews)
data_unlabeled2 = unlabeled_data[33013:]

X11_unlabeled=data_unlabeled1['rev_clean']
X22_unlabeled=data_unlabeled2['rev_clean']

# Feature extraction
vectorizer1 = TfidfVectorizer()  # TF-IDF vectorizer for view 1
vectorizer2 = TfidfVectorizer()  # TF-IDF vectorizer for view 2

X1_labeled = vectorizer1.fit_transform(X1_labeledd)
X2_labeled = vectorizer2.fit_transform(X2_labeledd)

X1_unlabeled = vectorizer1.transform(X11_unlabeled)
X2_unlabeled = vectorizer2.transform(X22_unlabeled)

# Initialize the classifiers for each view
classifier1 = xgb.XGBClassifier() # Classifier for view 1
classifier2 = xgb.XGBClassifier()   # Classifier for view 2
#classifier1 = xgb.XGBClassifier(max_depth=10, n_estimators=1000, learning_rate=0.05)
#classifier2 = xgb.XGBClassifier(max_depth=10, n_estimators=1000, learning_rate=0.05)

# Co-training iterations
alpha = 0.9  # Confidence threshold
max_iterations = 5

# Train the classifiers on the labeled data
classifier1.fit(X1_labeled, y_labeled1)
classifier2.fit(X2_labeled, y_labeled2)

y1_pred = classifier1.predict(X1_unlabeled)
y2_pred = classifier2.predict(X2_unlabeled)

# Predict probabilities for unlabeled data using the classifiers
clf1_probs = classifier1.predict_proba(X1_unlabeled)
clf2_probs = classifier2.predict_proba(X2_unlabeled)

# Identify samples with high confidence predictions from both classifiers
confident_samples1 = np.where(np.max(clf1_probs, axis=1) >= alpha)[0]
confident_samples2 = np.where(np.max(clf2_probs, axis=1) >= alpha)[0]

# Add high confidence samples to labeled data and remove from unlabeled data
X1_labeled = pd.concat([X1_labeledd, pd.Series([X11_unlabeled[i] for i in confident_samples1])], ignore_index=True)
y_labeled1 = pd.concat([y_labeled1, pd.Series(y1_pred[confident_samples1])], ignore_index=True)
unlabeled_reviews1 = [review for i, review in enumerate(X11_unlabeled) if i not in confident_samples1]

X2_labeled = pd.concat([X2_labeledd, X22_unlabeled.iloc[confident_samples2]], ignore_index=True)
y_labeled2 = pd.concat([y_labeled2, pd.Series(y2_pred[confident_samples2])], ignore_index=True)
unlabeled_reviews2 = [review for i, review in enumerate(X22_unlabeled) if i not in confident_samples2]

# Vectorize updated labeled data
X1_labeled = vectorizer1.transform(X1_labeled)
X2_labeled = vectorizer2.transform(X2_labeled)

# Retrain classifiers on updated labeled data
classifier1.fit(X1_labeled, y_labeled1)
classifier2.fit(X2_labeled, y_labeled2)

# Final predictions for the unlabeled data
X_unlabeled1 = vectorizer1.transform(unlabeled_reviews1)
X_unlabeled2 = vectorizer2.transform(unlabeled_reviews2)
y_unlabeled_pred1 = classifier1.predict(X_unlabeled1)
y_unlabeled_pred2 = classifier2.predict(X_unlabeled2)

X1_test = X_test[:301]  # Labeled data for view 1 (text reviews)
X2_test = X_test[301:]   # Labeled data for view 2 (additional information about the reviews)
y1_test =  y_test[:301]
y2_test =  y_test[301:]

X1_test = vectorizer1.transform(X1_test)
X2_test = vectorizer2.transform(X2_test)

y_pred1 = classifier1.predict(X1_test)
y_pred2 = classifier2.predict(X2_test)
ensemble_predictions = np.concatenate([y_pred1, y_pred2])

# Step 6: Model Evaluation

accuracy = accuracy_score(y_test, ensemble_predictions)
print("Accuracy: {:.2f}%".format(accuracy * 100))

from sklearn.metrics import precision_score, recall_score, f1_score
precision_labeled = precision_score(y_test, ensemble_predictions, average='weighted')
recall_labeled = recall_score(y_test, ensemble_predictions, average='weighted')
f1_labeled = f1_score(y_test, ensemble_predictions, average='weighted')
print("Precision Labeled Data :", precision_labeled)
print("Recall Labeled Data :", recall_labeled)
print("F1-Score Labeled Data :", f1_labeled)