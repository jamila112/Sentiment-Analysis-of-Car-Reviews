# -*- coding: utf-8 -*-
"""scrip1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YfTuu7_yE2nDc5pN_Py4p5HaJjQh_6uZ
"""

import requests
import lxml
from bs4 import BeautifulSoup
result = requests.get("https://www.caradisiac.com/constructeurs--automobiles/")
contents=result.content
soup = BeautifulSoup(contents,"lxml")
classes=soup.find_all("li",{"class":"constructeurVisuel logoMarque"})

l=[]
liens=[]
for i in range(len(classes)):
    liens.append(classes[i].find("a").attrs["href"])

nn="https://www.caradisiac.com"
list_avis=[]
pp=[]

for l in liens:
  ll=nn+l
  print("lien ",ll)
  result = requests.get(ll)
  src=result.content
  #print("src : ",src)
  soup = BeautifulSoup(src,"lxml")
  avis=soup.find("a",{"class":"twoLine"})
  print("avis ",avis)
  list_avis.append(avis)

hrefs=[]
for link in list_avis:
    href = link.get("href")
    hrefs.append(href)
    print(href)

nn="https://www.caradisiac.com"
urlss=[]
for l in hrefs:
  ll=nn+l
  print(ll)
  urlss.append(ll)

split_index_1 = 125
split_index_2 = 250
t1 = urlss[:split_index_1]   # Slice the first part of the array
t2 = urlss[split_index_1:split_index_2]   # Slice the second part of the array
t3 = urlss[split_index_2:]

print(len(t1))
print(len(t2))
print(len(t3))

#urlss=['https://www.caradisiac.com/auto--abarth/avis/','https://www.caradisiac.com/auto--ac/avis/','https://www.caradisiac.com/auto--audi/avis/']
#urlss=['https://www.caradisiac.com/auto--china-automobile/avis/']

comments=[]
modeles=[]
marques=[]
annee=[]
MM=[]

import requests
import time
from bs4 import BeautifulSoup
for s in t3:
    j=[]
    response = requests.get(s)
    print(response)
    soup = BeautifulSoup(response.content, "html.parser")
    page=soup.find("div",{"class":"bloc404"})
    avis=soup.find("p",{"class":"_text-justify_qn616_85 _text-normal_qn616_90 _text-medium_qn616_46 _margin-xlarge_qn616_172"})
    if page:
        print("page 404 found")
    else:
        if avis:
            print("avis not found")
        else:
            marque=soup.find("a", {"onclick": "return xt_click(this, 'C', 39, 'XTC_BlocNavMarque_Titre', 'N');"}).text.strip()

            #marque=soup.find("a", {"class": "return xt_click(this, 'C', 39, 'XTC_BlocNavMarque_Titre', 'N');"}).text.strip()
            #MARC= marque.split("SUR ")[1]
            print("la marqueeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee",marque)
            val_nav = soup.find("nav", {"class": "_pagination_1e74t_37"})
            if val_nav:
               #creation de liens de pages
               print("Link with value nav found.")
               link_tags = val_nav.find_all("a")
               links = [link.get("href") for link in link_tags]
               print(links)
               links.pop(-1)
               print(links)
               last_element = links[-1]
               print("last element :",last_element)
               number = int(last_element.split("_")[-1].rstrip("/"))
               urls=[]
               modele = last_element[1:last_element.find('/',1)]
               urls=[]
               n=number+1
               for i in range(1, n):
                 if i==1:
                     url = "/"+modele+"/avis/"
                     urls.append(url)
                 else:
                     url = "/"+modele+"/avis_{}/".format(i)
                     urls.append(url)
               nn="https://www.caradisiac.com"
               lin=[]
               for l in urls:
                 ll=nn+l
                 print("lien ",ll)
                 lin.append(ll)
               p=[]
               hhh=[]
               for l in lin:
                   print("l :",l)
                   result = requests.get(l)
                   src=result.content
                   soup = BeautifulSoup(src,"lxml")
                   hh=soup.find_all("h2",{"class":"_titreListingAvis_1h7nm_103"})
                   hhh.append(hh)
                   L=[]
                   years=[]
                   date_spans = soup.find_all('span', class_='Date')
                   date_texts = [date_span.text for date_span in date_spans]
                   print("zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz",len(date_texts))
                   print("kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk",date_texts)
                   L.append(date_texts)
                   result = sum(L, [])
                   for date in result:
                                  first_year = date[date.find("(") + 1 : date.find(" ") + 5]
                                 #print(first_year)
                                  year = first_year[-4:]
                                  print("year",year)
                                  years.append(year)

                   print("years",years)
                   annee.append(years)
               print("*********************************************************************************************************")
               souppp = BeautifulSoup(''.join(str(e) for e in hhh), 'html.parser')
               linkk = [a['href'] for a in souppp.find_all('a')]
               for p in linkk:
                   result = requests.get(p)
                   src=result.content
                   soup = BeautifulSoup(src,"lxml")
                   val_nav = soup.find("nav", {"class": "_pagination_1e74t_37"})
                   if val_nav:
                            link_tags = val_nav.find_all("a")
                            links = [link.get("href") for link in link_tags]
                            print("links                ",links)
                            links.pop(-1)
                            last_element = links[-1]
                            print("last element :",last_element)
                            number = int(last_element.split("_")[-1].rstrip("/"))
                            modele = last_element[1:last_element.find('/',1)]
                            urls=[]
                            num=number+1
                            for i in range(1, num):
                                 if i==1:
                                     url = "/"+modele+"/avis/"
                                     urls.append(url)
                                 else:
                                     url = "/"+modele+"/avis_{}/".format(i)
                                     urls.append(url)
                            nn="https://www.caradisiac.com"
                            lin=[]
                            M=0
                            for l in urls:
                                 ll=nn+l

                                 result11 = requests.get(ll)
                                 src=result11.content
                                 soup = BeautifulSoup(src,"lxml")
                                 soup4=soup.find_all("p",{"class":"_text-left_qn616_70 _text-normal_qn616_90 _text-medium_qn616_46 _truncate3_qn616_112 _margin-none_qn616_156"})
                                 for p in soup4:
                                       text=p.get_text()
                                       mod=soup.find("h1", {"class": "_titre1_1h7nm_40"}).text.strip()
                                       m = mod.split("AVIS ")[1]
                                       print("Voir le modele ::::",m)
                                       #print("text  -----------------------------------------------------------",text)
                                       marques.append(marque)
                                       modeles.append(m)
                                       M=M+1
                                       comments.append(text)
                            print("mmmmmmmmmmmmmmmmmmm111111111 =",M)
                            MM.append(M)

                   else:
                           soup4=soup.find_all("p",{"class":"_text-left_qn616_70 _text-normal_qn616_90 _text-medium_qn616_46 _truncate3_qn616_112 _margin-none_qn616_156"})
                           M=0
                           for p in soup4:
                               text=p.get_text()
                               mod=soup.find("h1", {"class": "_titre1_1h7nm_40"}).text.strip()
                               m = mod.split("AVIS ")[1]
                               print("Voir le modele ::::",m)
                               #print("text  -----------------------------------------------------------",text)
                               marques.append(marque)
                               modeles.append(m)
                               M=M+1
                               comments.append(text)
                           print("mmmmmmmmmmmmmmmmmmmmmmmmmmmmm222222222222222 =",M)
                           MM.append(M)

            else:
                L=[]
                years=[]
                date_spans = soup.find_all('span', class_='Date')
                date_texts = [date_span.text for date_span in date_spans]
                print("zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz",len(date_texts))
                print("kkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkkk",date_texts)
                L.append(date_texts)
                result = sum(L, [])
                for date in result:
                                  first_year = date[date.find("(") + 1 : date.find(" ") + 5]
                                  #print(first_year)
                                  year = first_year[-4:]
                                  print("year",year)
                                  years.append(year)

                print("years",years)
                annee.append(years)
                ha=soup.find_all("h2",{"class":"_titreListingAvis_1h7nm_103"})
                j.append(ha)
                print("*********************************************************************************************************")
                sou = BeautifulSoup(''.join(str(e) for e in j), 'html.parser')
                lll = [a['href'] for a in sou.find_all('a')]
                for lu in lll:

                       result = requests.get(lu)
                       src=result.content
                       soup = BeautifulSoup(src,"lxml")
                       val_nav = soup.find("nav", {"class": "_pagination_1e74t_37"})
                       if val_nav:
                            link_tags = val_nav.find_all("a")
                            links = [link.get("href") for link in link_tags]
                            #print("links                ",links)
                            links.pop(-1)
                            last_element = links[-1]
                            number = int(last_element.split("_")[-1].rstrip("/"))
                            modele = last_element[1:last_element.find('/',1)]
                            urls=[]
                            num=number+1
                            for i in range(1, num):
                                 if i==1:
                                     url = "/"+modele+"/avis/"
                                     urls.append(url)
                                 else:
                                     url = "/"+modele+"/avis_{}/".format(i)
                                     urls.append(url)
                            nn="https://www.caradisiac.com"
                            lin=[]
                            M=0
                            for l in urls:

                                 ll=nn+l
                                 result11 = requests.get(ll)
                                 src=result11.content
                                 soup = BeautifulSoup(src,"lxml")
                                 soup4=soup.find_all("p",{"class":"_text-left_qn616_70 _text-normal_qn616_90 _text-medium_qn616_46 _truncate3_qn616_112 _margin-none_qn616_156"})
                                 for p in soup4:
                                       text=p.get_text()
                                       mod=soup.find("h1", {"class": "_titre1_1h7nm_40"}).text.strip()
                                       m = mod.split("AVIS ")[1]
                                       print("Voir le modele ::::",m)
                                       #print("text  -----------------------------------------------------------",text)
                                       marques.append(marque)
                                       modeles.append(m)
                                       M=M+1
                                       comments.append(text)

                            print("mmmmmmmmmmmmmmmmmmmmm333333333333333 =",M)
                            MM.append(M)


                       else:
                           soup4=soup.find_all("p",{"class":"_text-left_qn616_70 _text-normal_qn616_90 _text-medium_qn616_46 _truncate3_qn616_112 _margin-none_qn616_156"})
                           M=0
                           for p in soup4:
                               text=p.get_text()
                               mod=soup.find("h1", {"class": "_titre1_1h7nm_40"}).text.strip()
                               m = mod.split("AVIS ")[1]
                               print("Voir le modele ::::",m)
                               #print("text  -----------------------------------------------------------",text)
                               marques.append(marque)
                               modeles.append(m)
                               M=M+1
                               comments.append(text)
                           print("mmmmmmmmmmmmmmmmmmmmmmmmmmmmm4444444444444444444 =",M)
                           MM.append(M)
            print(" attttttttttttttttttttttttttttttttttttttttttttttttttend")



print("MM",MM)
print("annee ",annee)
ane = [item for sublist in annee for item in sublist]
print("ane ",ane)
print(len(ane))
list3 = [elem for i, elem in enumerate(ane) for j in range(MM[i])]
print(len(list3))
print(list3)
#######################################################""
print(len(comments))
print(len(modeles))
print(len(list3))
print(len(marques))

import openpyxl

wb = openpyxl.Workbook()
ws = wb.active

ws.append(['makes', 'modeles', 'comments','years'])

for i in range(len(comments)):
    ws.append([marques[i], modeles[i], comments[i],list3[i]])

wb.save('data1.xlsx')