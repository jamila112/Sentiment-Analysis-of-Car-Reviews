# -*- coding: utf-8 -*-
"""DATA_CLEAN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UmYdtJDm07Bhke0D7qjHi-P_rcozMKhX
"""

!python -m spacy download fr_core_news_md

import nltk
import string
import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import spacy
import string
import spacy
from spacy.lang.fr.stop_words import STOP_WORDS as fr_stopwords
import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import spacy
import string
import spacy
from spacy.lang.fr.stop_words import STOP_WORDS as fr_stopwords

nltk.download('wordnet')
nltk.download('punkt')
nltk.download('stopwords')
# Importer les librairies
import nltk
from nltk.tokenize import word_tokenize
import pandas as pd
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import re
import pandas as pd
from spacy import displacy
import spacy

nltk.download('omw-1.4')
from nltk.sentiment.vader import SentimentIntensityAnalyzer
nltk.download('vader_lexicon')

import spacy
nlp = spacy.load('fr_core_news_md')

#######################################################################################

from pandas.core.groupby import DataFrameGroupBy
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_excel('/content/sample_data/data_cars_complet.xlsx')

import pandas as pd
import matplotlib.pyplot as plt

# Assuming you have a DataFrame named 'data' with columns 'makes' and 'reviews'

# Calculate the count of reviews per car make
make_reviews_count = df['makes'].value_counts()

# Sort the makes based on review counts
sorted_makes = make_reviews_count.sort_values(ascending=False)

# Select the top 10 makes
top_10_makes = sorted_makes.head(10)

# Create a bar chart of the top 10 makes
top_10_makes.plot(kind='bar')

# Add labels and title to the chart
plt.xlabel('Makes')
plt.ylabel('Count of Reviews')
plt.title('the 10 Cars with Highest Count of Reviews')

# Display the chart
plt.show()

#################################################################################################""

import string
import spacy
from spacy.lang.fr.stop_words import STOP_WORDS as fr_stopwords
negation_words = {'pas', 'ne'}
fr_stopwords = fr_stopwords - negation_words
nlp = spacy.load('fr_core_news_md')

df = pd.read_excel('/content/sample_data/unlabeled_data.xlsx')
df.head(6)

columns_to_drop = ['rev_clean']
df.drop(columns_to_drop, axis=1, inplace=True)

import pandas as pd
import re
from nltk.corpus import stopwords
from nltk.stem.snowball import SnowballStemmer

df.dropna(inplace=True)

df.drop_duplicates(inplace=True)

df['rev_clean'] = df['comments'].apply(lambda x: word_tokenize(x.lower()))

stop_words = set(stopwords.words('french'))
df['rev_clean'] = df['rev_clean'].apply(lambda x: [word for word in x if word not in stop_words])

# Remove punctuation, digits, URLs, and emoticons
def preprocess_text(text):
    # Remove punctuation
    text = re.sub(r'[^\w\s]', '', text)
    # Remove URLs
    text = re.sub(r'http\S+', '', text)
    # Remove emoticons
    text = re.sub(r'[^\w\s,]', '', text)
    return text

df['rev_clean'] = df['rev_clean'].astype(str)

df['rev_clean'] = df['rev_clean'].apply(preprocess_text)

l=df['rev_clean']
lematized=[]
for s in l:
  do = nlp(s)
  lemmatized_sentence = ' '.join([token.lemma_ for token in do])
  #print(lemmatized_sentence)
  lematized.append(lemmatized_sentence)

df['rev_clean']=lematized

df.to_excel('unlabeled_data_new.xlsx')